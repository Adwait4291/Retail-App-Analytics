{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4495910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import quote_plus\n",
    "import urllib.parse\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efe0a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get MongoDB connection details from environment variables\n",
    "username = os.getenv(\"MONGODB_USERNAME\")\n",
    "password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "cluster = os.getenv(\"MONGODB_CLUSTER\")\n",
    "database = os.getenv(\"MONGODB_DATABASE\")\n",
    "escaped_username = urllib.parse.quote_plus(username)\n",
    "escaped_password = urllib.parse.quote_plus(password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd292554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to MongoDB...\n",
      "MongoDB Configuration Error: The DNS query name does not exist: _mongodb._tcp.retailcluster.0mqnqed.mongodb.net.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient, errors # Import errors for specific exception handling\n",
    "import os # Optional: Good practice for handling paths\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. MongoDB Connection Details (Replace with your actual details)\n",
    "#    Get this from MongoDB Atlas (Connect -> Drivers) or your local setup\n",
    "#    Example format: \"mongodb+srv://<username>:<password>@your_cluster_address/?retryWrites=true&w=majority\"\n",
    "MONGO_CONNECTION_STRING = \"mongodb+srv://retail:retail123@retailcluster.ipxcovl.mongodb.net/\"\n",
    "DATABASE_NAME = \"detaildb\"  # Replace with your database name (e.g., \"retail_db\")\n",
    "COLLECTION_NAME = \"retail_app_data\" # Replace with your collection name (e.g., \"sales_data\")\n",
    "\n",
    "# 2. CSV File Path (Corrected Formatting)\n",
    "#    Using a raw string (r\"...\") is recommended on Windows to handle backslashes correctly.\n",
    "csv_file_path = r\"D:\\DATA AND AI\\ML WORKSHOP\\Session 2 Data Processing-20250316T042803Z-001\\Session 2 Data Processing\\Data and code\\retail_app_data.csv\"\n",
    "#    Alternatively, you could use forward slashes \"/\":\n",
    "# csv_file_path = \"D:/DATA AND AI/ML WORKSHOP/Session 2 Data Processing-20250316T042803Z-001/Session 2 Data Processing/Data and code/retail_app_data.csv\"\n",
    "\n",
    "\n",
    "# --- Main Script Logic ---\n",
    "client = None # Initialize client to None to ensure it can be used in the finally block\n",
    "try:\n",
    "    # --- Establish MongoDB Connection ---\n",
    "    print(f\"Attempting to connect to MongoDB...\")\n",
    "    # Set a server selection timeout to handle connection errors faster (e.g., 5 seconds)\n",
    "    client = MongoClient(MONGO_CONNECTION_STRING, serverSelectionTimeoutMS=5000)\n",
    "\n",
    "    # The ismaster command is cheap and does not require auth. It forces a connection check.\n",
    "    client.admin.command('ismaster')\n",
    "    print(\"MongoDB connection successful!\")\n",
    "\n",
    "    # Select the database\n",
    "    db = client[DATABASE_NAME]\n",
    "    print(f\"Selected database: '{DATABASE_NAME}'\")\n",
    "\n",
    "    # Select the collection\n",
    "    collection = db[COLLECTION_NAME]\n",
    "    print(f\"Selected collection: '{COLLECTION_NAME}'\")\n",
    "\n",
    "    # --- Read CSV File ---\n",
    "    print(f\"Reading CSV file from: {csv_file_path}\")\n",
    "    # Check if the file exists before attempting to read\n",
    "    if not os.path.exists(csv_file_path):\n",
    "        raise FileNotFoundError(f\"Error: CSV file not found at the specified path: {csv_file_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    print(f\"Successfully read {len(df)} rows from the CSV file.\")\n",
    "\n",
    "    # --- Prepare Data for MongoDB ---\n",
    "    # Convert DataFrame to a list of dictionaries (each dictionary is a MongoDB document)\n",
    "    # Ensure CSV column names are valid for MongoDB keys (e.g., no '.' or '$' at the start)\n",
    "    if df.empty:\n",
    "        print(\"CSV file is empty. No records to insert.\")\n",
    "        records = []\n",
    "    else:\n",
    "        # Handle potential NaN values which are not directly supported by BSON/MongoDB\n",
    "        # Replace pandas NaN with None, which MongoDB handles correctly\n",
    "        df_filled = df.fillna(value=float('nan')).replace({float('nan'): None})\n",
    "        records = df_filled.to_dict(\"records\")\n",
    "        print(f\"Converted DataFrame to {len(records)} records (NaN values replaced with None).\")\n",
    "\n",
    "    # --- Insert Data into MongoDB ---\n",
    "    if records: # Only proceed if there are records to insert\n",
    "        print(f\"Inserting {len(records)} records into MongoDB collection '{COLLECTION_NAME}'...\")\n",
    "        # insert_many inserts multiple documents from the list\n",
    "        result = collection.insert_many(records)\n",
    "        print(f\"Successfully inserted {len(result.inserted_ids)} documents.\")\n",
    "\n",
    "        # --- Verify Insertion Count ---\n",
    "        # Count total documents in the collection after insertion\n",
    "        total_docs = collection.count_documents({})\n",
    "        print(f\"Collection '{COLLECTION_NAME}' now contains {total_docs} documents.\")\n",
    "    else:\n",
    "        print(\"Skipping insertion as there are no records.\")\n",
    "\n",
    "# --- Error Handling ---\n",
    "except FileNotFoundError as fnf_error:\n",
    "    # Handle the specific case where the CSV file isn't found\n",
    "    print(f\"File Error: {fnf_error}\")\n",
    "except errors.ConfigurationError as config_error:\n",
    "    # Handle errors related to MongoDB connection string format or options\n",
    "     print(f\"MongoDB Configuration Error: {config_error}\")\n",
    "except errors.ConnectionFailure as conn_error:\n",
    "    # Handle errors failing to connect to MongoDB server\n",
    "    print(f\"MongoDB Connection Error: Could not connect to the server. Check connection string, network access, and server status. Details: {conn_error}\")\n",
    "except Exception as e:\n",
    "    # Catch any other unexpected errors during the process\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    # Consider logging the full traceback here for debugging complex issues\n",
    "    # import traceback\n",
    "    # print(traceback.format_exc())\n",
    "finally:\n",
    "    # Ensure the MongoDB connection is closed even if errors occurred\n",
    "    if client:\n",
    "        print(\"Closing MongoDB connection.\")\n",
    "        client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88394f5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbf0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9fb89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df_processed = df.copy()    \n",
    "df_processed.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea6aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Time-based Processing\n",
    "# Convert datetime columns\n",
    "df_processed['first_visit_date'] = pd.to_datetime(df_processed['first_visit_date'])\n",
    "df_processed['purchase_date'] = pd.to_datetime(df_processed['purchase_date'])\n",
    "\n",
    "# Calculate time difference and create target\n",
    "df_processed['time_to_purchase'] = (df_processed['purchase_date'] - \n",
    "                                  df_processed['first_visit_date']).dt.total_seconds() / 3600\n",
    "\n",
    "# Create 24-hour purchase target\n",
    "df_processed['purchase_24h'] = np.where(df_processed['time_to_purchase'] <= 24, 1, 0)\n",
    "\n",
    "# Extract time features\n",
    "df_processed['hour'] = df_processed['first_visit_date'].dt.hour\n",
    "df_processed['dayofweek'] = df_processed['first_visit_date'].dt.dayofweek\n",
    "df_processed['is_weekend'] = df_processed['dayofweek'].isin([5,6]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058451a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0a3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aeb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Screen List Processing\n",
    "# Add comma for consistent processing\n",
    "df_processed['screen_list'] = df_processed['screen_list'].astype(str) + ','\n",
    "\n",
    "# Define screen categories\n",
    "shopping_screens = ['ProductList', 'ProductDetail', 'CategoryBrowse', 'Search']\n",
    "cart_screens = ['ShoppingCart', 'Checkout', 'PaymentMethods', 'DeliveryOptions']\n",
    "engagement_screens = ['WishList', 'Reviews', 'Promotions']\n",
    "account_screens = ['Account', 'AddressBook', 'OrderTracking']\n",
    "\n",
    "# Create binary indicators for each screen\n",
    "for screen in (shopping_screens + cart_screens + engagement_screens + account_screens):\n",
    "    df_processed[screen.lower()] = df_processed['screen_list'].str.contains(screen).astype(int)\n",
    "\n",
    "# Create count features for each category\n",
    "df_processed['shopping_count'] = df_processed[[s.lower() for s in shopping_screens]].sum(axis=1)\n",
    "df_processed['cart_count'] = df_processed[[s.lower() for s in cart_screens]].sum(axis=1)\n",
    "df_processed['engagement_count'] = df_processed[[s.lower() for s in engagement_screens]].sum(axis=1)\n",
    "df_processed['account_count'] = df_processed[[s.lower() for s in account_screens]].sum(axis=1)\n",
    "\n",
    "# Create Other category\n",
    "all_tracked_screens = shopping_screens + cart_screens + engagement_screens + account_screens\n",
    "df_processed['other_screens'] = df_processed['screen_list'].apply(\n",
    "    lambda x: len([s for s in x.split(',') if s and s not in all_tracked_screens])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200000)\n",
    "df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020dd2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_processed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f4fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed[['user_id',\t'first_visit_date',\t'age','added_to_wishlist', 'made_purchase', 'purchase_date', 'shoppingcart', 'checkout', 'paymentmethods', 'deliveryoptions', 'wishlist', 'reviews', 'promotions', 'account']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d923948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Feature Engineering\n",
    "# Create engagement score\n",
    "df_processed['engagement_score'] = (\n",
    "    df_processed['session_count'] * 0.3 +\n",
    "    df_processed['used_search_feature'] * 0.2 +\n",
    "    df_processed['wrote_review'] * 0.15 +\n",
    "    df_processed['added_to_wishlist'] * 0.15 +\n",
    "    df_processed['total_screens_viewed'] * 0.2\n",
    ")\n",
    "\n",
    "# Create screen diversity score\n",
    "df_processed['screen_diversity'] = (\n",
    "    df_processed[['shopping_count', 'cart_count', \n",
    "                 'engagement_count', 'account_count']].gt(0).sum(axis=1)\n",
    ")\n",
    "\n",
    "# Create purchase intent score\n",
    "df_processed['purchase_intent'] = (\n",
    "    df_processed['cart_count'] * 0.4 +\n",
    "    df_processed['shopping_count'] * 0.3 +\n",
    "    df_processed['engagement_count'] * 0.2 +\n",
    "    df_processed['added_to_wishlist'] * 0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f1e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Categorical Feature Processing\n",
    "# Platform encoding (keeping existing)\n",
    "df_processed['platform'] = df_processed['platform'].map({'iOS': 1, 'Android': 0})\n",
    "\n",
    "# Process new categorical columns\n",
    "# Region encoding\n",
    "region_dummies = pd.get_dummies(df_processed['region'], prefix='region')\n",
    "df_processed = pd.concat([df_processed, region_dummies], axis=1)\n",
    "\n",
    "# Acquisition channel encoding\n",
    "channel_dummies = pd.get_dummies(df_processed['acquisition_channel'], prefix='channel')\n",
    "df_processed = pd.concat([df_processed, channel_dummies], axis=1)\n",
    "\n",
    "# User segment processing\n",
    "# Extract age group and user type separately for more granular analysis\n",
    "df_processed['age_group'] = df_processed['user_segment'].apply(lambda x: x.split()[0])\n",
    "df_processed['user_type'] = df_processed['user_segment'].apply(lambda x: ' '.join(x.split()[1:]))\n",
    "\n",
    "age_group_dummies = pd.get_dummies(df_processed['age_group'], prefix='age_group')\n",
    "user_type_dummies = pd.get_dummies(df_processed['user_type'], prefix='user_type')\n",
    "df_processed = pd.concat([df_processed, age_group_dummies, user_type_dummies], axis=1)\n",
    "\n",
    "# App version processing\n",
    "# Extract major version for simplified analysis\n",
    "df_processed['app_major_version'] = df_processed['app_version'].apply(lambda x: int(x.split('.')[0]))\n",
    "\n",
    "# Create version recency score (higher = newer version)\n",
    "df_processed['version_score'] = df_processed['app_version'].apply(\n",
    "    lambda x: sum(float(i)/(10**n) for n, i in enumerate(x.split('.')))\n",
    ")\n",
    "\n",
    "# 6. Clean up and prepare final dataset\n",
    "# Drop original columns that have been processed\n",
    "columns_to_drop = [\n",
    "    'screen_list', 'purchase_date', 'first_visit_date', \n",
    "    'time_to_purchase', 'made_purchase', 'region', \n",
    "    'acquisition_channel', 'user_segment', 'app_version',\n",
    "    'age_group', 'user_type'\n",
    "]\n",
    "df_processed = df_processed.drop(columns=columns_to_drop)\n",
    "\n",
    "# Ensure all column names are lowercase\n",
    "df_processed.columns = df_processed.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f3c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Quality Checks\n",
    "print(\"Data Quality Report\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Shape: {df_processed.shape}\")\n",
    "print(f\"\\nNull values:\\n{df_processed.isnull().sum()[df_processed.isnull().sum() > 0]}\")\n",
    "print(f\"\\nPurchase rate (24h): {df_processed['purchase_24h'].mean():.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb78127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Feature Correlations\n",
    "correlation_matrix = df_processed.corr()['purchase_24h'].sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Features by Correlation with Purchase:\")\n",
    "print(correlation_matrix[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# --- Configuration for writing back to MongoDB ---\n",
    "DATABASE_NAME_WRITE = \"detaildb\"\n",
    "COLLECTION_NAME_WRITE = \"processed_retail_app_data\"  # Use a new collection name for processed data\n",
    "\n",
    "try:\n",
    "    # --- Reuse existing MongoDB connection ---\n",
    "    if client:\n",
    "        # Select the database and collection for writing\n",
    "        db_write = client[DATABASE_NAME_WRITE]\n",
    "        collection_write = db_write[COLLECTION_NAME_WRITE]\n",
    "\n",
    "        # --- Prepare Processed Data for MongoDB ---\n",
    "        if not df_processed.empty:\n",
    "            df_processed_filled = df_processed.fillna(value=float('nan')).replace({float('nan'): None})\n",
    "            records_to_insert = df_processed_filled.to_dict(\"records\")\n",
    "\n",
    "            # --- Insert Processed Data into MongoDB ---\n",
    "            if records_to_insert:\n",
    "                result_insert = collection_write.insert_many(records_to_insert)\n",
    "                print(f\"Successfully inserted processed data into '{COLLECTION_NAME_WRITE}'.\")\n",
    "            else:\n",
    "                print(\"No processed records to insert.\")\n",
    "        else:\n",
    "            print(\"Processed DataFrame is empty. No data to write.\")\n",
    "    else:\n",
    "        print(\"Error: MongoDB client connection was not established.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during writing to MongoDB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdf6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_mongodb(df):\n",
    "    \"\"\"\n",
    "    Save processed dataframe to MongoDB.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Processed dataframe to save\n",
    "    \"\"\"\n",
    "    print(\"\\nSaving processed data to MongoDB...\")\n",
    "    \n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get MongoDB connection details from environment variables\n",
    "    username = os.getenv(\"MONGODB_USERNAME\")\n",
    "    password = os.getenv(\"MONGODB_PASSWORD\")\n",
    "    cluster = os.getenv(\"MONGODB_CLUSTER\")\n",
    "    database = os.getenv(\"MONGODB_DATABASE\")\n",
    "    \n",
    "    # Create connection string\n",
    "    connection_string = f\"mongodb+srv://{username}:{password}@{cluster}/\"\n",
    "    \n",
    "    try:\n",
    "        # Create a client connection\n",
    "        client = MongoClient(connection_string)\n",
    "        \n",
    "        # Connect to the database\n",
    "        db = client.get_database(database)\n",
    "        collection = db.processed_data  # Collection for processed data\n",
    "        \n",
    "        # Convert DataFrame to dictionary records\n",
    "        records = df.to_dict(\"records\")\n",
    "        \n",
    "        # Clear existing records and insert new ones\n",
    "        collection.delete_many({})\n",
    "        result = collection.insert_many(records)\n",
    "        \n",
    "        print(f\"Successfully saved {len(result.inserted_ids)} processed records to MongoDB\")\n",
    "        print(f\"Database: {database}, Collection: processed_data\")\n",
    "        \n",
    "        # Display sample of the saved data\n",
    "        print(\"\\nSample of saved processed data (first 3 records):\")\n",
    "        for doc in collection.find().limit(3):\n",
    "            print(f\"User ID: {doc['user_id']}, Purchase 24h: {doc['purchase_24h']}, Engagement Score: {doc.get('engagement_score', 'N/A')}\")\n",
    "        \n",
    "        # Close the connection\n",
    "        client.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data to MongoDB: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4fc548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retailapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
